/**
 * M√≥dulo: üß† agente.js
 * Projeto: üìò AI Game Learning
 *
 * Este m√≥dulo define o Agente que utiliza o algoritmo Q-Learning.
 * Ele √© projetado para ser compat√≠vel tanto com o treinamento em massa (treinador.js)
 * quanto com o aprendizado interativo (jogar.js).
 */

import fs from 'fs';
import path from 'path';

/**
 * Um Agente que aprende a jogar Jogo da Velha usando Q-Learning.
 * 
 * Pense neste Agente como um jogador de Ragnarok Online que est√° aprendendo
 * a melhor estrat√©gia para derrotar monstros.
 *
 * Hiperpar√¢metros (os "atributos" do nosso jogador):
 * - alpha (Œ±): A "Velocidade de Aprendizado"
 *   - Qu√£o r√°pido o jogador ajusta sua estrat√©gia ap√≥s uma batalha
 *   - Valores altos = impulsivo, aprende r√°pido com uma √∫nica experi√™ncia
 *   - Valores baixos = c√©tico, precisa de muitas experi√™ncias para mudar de ideia
 *
 * - gamma (Œ≥): A "Vis√£o de Futuro" (Fator de Desconto)
 *   - O quanto o jogador valoriza recompensas futuras
 *   - Valor alto = estrategista, pensa nos pr√≥ximos passos
 *   - Valor baixo = imediatista, foca apenas na recompensa de agora
 *
 * - epsilon (Œµ): O "Medidor de Curiosidade" (Taxa de Explora√ß√£o)
 *   - A chance do jogador tentar uma t√°tica nova e desconhecida
 *   - Valor alto = aventureiro, adora explorar o mapa
 *   - Valor baixo = conservador, prefere usar a t√°tica que j√° sabe que funciona
 * 
 * @property {number} alpha - Taxa de aprendizado (0 a 1)
 * @property {number} gamma - Fator de desconto (0 a 1)
 * @property {number} epsilon - Taxa de explora√ß√£o (0 a 1)
 * @property {number} epsilonMinimo - Valor m√≠nimo que epsilon pode atingir
 * @property {number} taxaDecaimentoEpsilon - Multiplicador de decaimento do epsilon
 * @property {number} jogador - Identificador do jogador (1 ou 2)
 * @property {string} simbolo - S√≠mbolo visual ('X' ou 'O')
 * @property {Object} tabelaQ - Mem√≥ria do agente (Q-Table)
 * @property {number} partidasTreinadas - Contador de partidas de treino
 * @property {number} vitorias - Contador de vit√≥rias
 * @property {number} derrotas - Contador de derrotas
 * @property {number} empates - Contador de empates
 * @property {Array<Array>} historicoPartida - Mem√≥ria de curto prazo da partida atual
 */
export class AgenteQLearning {
  /**
   * Inicializa os atributos e a mem√≥ria do Agente.
   * 
   * √â como criar um personagem novo no Ragnarok: voc√™ define seus atributos iniciais
   * (INT, DEX, etc.) que v√£o determinar como ele vai evoluir durante o jogo.
   * 
   * @param {Object} config - Objeto de configura√ß√£o com os hiperpar√¢metros do agente
   * @param {number} [config.alpha=0.5] - Taxa de aprendizado (0 a 1)
   * @param {number} [config.gamma=1.0] - Fator de desconto (0 a 1)
   * @param {number} [config.epsilon=1.0] - Taxa de explora√ß√£o inicial (0 a 1)
   * @param {number} [config.epsilonMinimo=0.001] - Valor m√≠nimo que epsilon pode atingir
   * @param {number} [config.taxaDecaimentoEpsilon=0.99999] - Multiplicador de decaimento do epsilon
   * @param {number} [config.jogador=1] - Identificador do jogador (1 para X, 2 para O)
   */
  constructor({
    alpha = 0.5,
    gamma = 1.0,
    epsilon = 1.0,
    epsilonMinimo = 0.001,
    taxaDecaimentoEpsilon = 0.99999,
    jogador = 1
  } = {}) {
    // --- HIPERPAR√ÇMETROS (Atributos do Agente) ---
    this.alpha = alpha;
    this.gamma = gamma;
    this.epsilon = epsilon;
    this.epsilonMinimo = epsilonMinimo;
    this.taxaDecaimentoEpsilon = taxaDecaimentoEpsilon;

    // --- IDENTIDADE ---
    this.jogador = jogador;
    this.simbolo = jogador === 1 ? 'X' : 'O';

    // --- MEM√ìRIA (A "Enciclop√©dia de Monstros" do Jogador) ---
    this.tabelaQ = {};

    // --- ESTAT√çSTICAS DE TREINO ---
    this.partidasTreinadas = 0;
    this.vitorias = 0;
    this.derrotas = 0;
    this.empates = 0;

    // --- MEM√ìRIA DE CURTO PRAZO (para a partida atual) ---
    this.historicoPartida = [];
  }

  /**
   * Consulta a "mem√≥ria" (Q-Table) para ver o valor de uma a√ß√£o em um estado.
   * 
   * Se o Agente nunca viu essa situa√ß√£o antes, ele assume que o valor √© 0,
   * como um jogador que nunca enfrentou aquele tipo de monstro e n√£o sabe
   * se a estrat√©gia ser√° boa ou ruim.
   * 
   * @param {string} estado - O estado atual do jogo (representa√ß√£o do tabuleiro)
   * @param {number} acao - A a√ß√£o (posi√ß√£o de 0 a 8) que o agente quer avaliar
   * @returns {number} O valor Q da a√ß√£o naquele estado (recompensa esperada)
   */
  obterValorQ(estado, acao) {
    if (!(estado in this.tabelaQ)) {
      this.tabelaQ[estado] = {};
    }
    if (!(acao in this.tabelaQ[estado])) {
      this.tabelaQ[estado][acao] = 0.0;
    }
    return this.tabelaQ[estado][acao];
  }

  /**
   * Atualiza a "mem√≥ria" do Agente usando a Equa√ß√£o de Bellman (M√©todo TD Learning).
   * √â aqui que o aprendizado a cada passo realmente acontece!
   * 
   * Pense como se fosse ganhar EXP no Ragnarok: depois de uma batalha,
   * voc√™ aprende se aquela estrat√©gia foi boa ou ruim e ajusta sua
   * "experi√™ncia" com base na recompensa que recebeu e no que espera
   * das pr√≥ximas batalhas.
   * 
   * A Equa√ß√£o de Bellman calcula:
   * Novo Valor Q = Opini√£o Antiga + Taxa de Aprendizado √ó (Surpresa)
   * Onde Surpresa = Valor Real - Opini√£o Antiga
   * 
   * @param {string} estado - O estado do tabuleiro antes da jogada
   * @param {number} acao - A a√ß√£o (posi√ß√£o 0-8) que foi tomada
   * @param {number} recompensa - A recompensa recebida (+1, -1 ou 0)
   * @param {string} proximoEstado - O estado do tabuleiro ap√≥s a jogada
   * @param {boolean} finalizado - Se true, n√£o considera valores futuros (jogo acabou)
   * @returns {void}
   */
  aprender(estado, acao, recompensa, proximoEstado, finalizado) {
    const opiniaoAntiga = this.obterValorQ(estado, acao);
    
    // Se o jogo finalizou, n√£o h√° valor futuro a considerar
    const melhorValorFuturo = finalizado ? 0.0 : this.#obterMelhorValorQDoEstado(proximoEstado);
    
    const valorRealDaJogada = recompensa + this.gamma * melhorValorFuturo;
    const surpresa = valorRealDaJogada - opiniaoAntiga;
    const novoValorQ = opiniaoAntiga + this.alpha * surpresa;

    if (!(estado in this.tabelaQ)) {
      this.tabelaQ[estado] = {};
    }
    this.tabelaQ[estado][acao] = novoValorQ;
  }

  /**
   * Verifica na "mem√≥ria" qual √© a melhor jogada poss√≠vel a partir de um estado.
   * 
   * √â como se o Agente olhasse todas as t√°ticas que ele j√° testou naquela
   * situa√ß√£o e escolhesse aquela que teve o melhor resultado no passado.
   * Se ele nunca viu aquela situa√ß√£o antes, retorna 0 (neutro).
   * 
   * @private
   * @param {string} estado - O estado do tabuleiro que queremos avaliar
   * @returns {number} O maior valor Q encontrado para aquele estado (melhor recompensa esperada)
   */
  #obterMelhorValorQDoEstado(estado) {
    if (!(estado in this.tabelaQ) || Object.keys(this.tabelaQ[estado]).length === 0) {
      return 0.0;
    }
    return Math.max(...Object.values(this.tabelaQ[estado]));
  }

  /**
   * Decide qual jogada fazer usando a estrat√©gia Epsilon-Greedy.
   * 
   * √â a estrat√©gia que equilibra "Aventura" (explora√ß√£o) e "Farm" (explora√ß√£o).
   * Como um jogador de Ragnarok que √†s vezes sai do caminho conhecido para
   * explorar novos mapas (pode encontrar algo melhor) ou fica no mapa conhecido
   * farmando o que j√° sabe que funciona.
   * 
   * A estrat√©gia funciona assim:
   * - Durante o treinamento: com probabilidade epsilon (Œµ), escolhe a√ß√£o aleat√≥ria
   *   (explora√ß√£o). Caso contr√°rio, escolhe a melhor a√ß√£o conhecida (explora√ß√£o).
   * - Fora do treinamento: sempre escolhe a melhor a√ß√£o conhecida.
   * 
   * @param {string} estado - O estado atual do tabuleiro
   * @param {Array<number>} acoesValidas - Lista de posi√ß√µes dispon√≠veis para jogar (0-8)
   * @param {boolean} [emTreinamento=true] - Se true, usa epsilon-greedy. Se false, sempre escolhe a melhor a√ß√£o
   * @returns {number} A a√ß√£o escolhida (posi√ß√£o de 0 a 8 no tabuleiro)
   * @throws {Error} Se n√£o houver a√ß√µes v√°lidas dispon√≠veis
   */
  escolherAcao(estado, acoesValidas, emTreinamento = true) {
    if (!acoesValidas || acoesValidas.length === 0) {
      throw new Error("N√£o h√° a√ß√µes v√°lidas para escolher.");
    }

    if (!emTreinamento) {
      return this.#escolherMelhorAcao(estado, acoesValidas);
    }

    if (Math.random() < this.epsilon) {
      // "Modo Aventura": explora
      return acoesValidas[Math.floor(Math.random() * acoesValidas.length)];
    } else {
      // "Modo Farm": usa melhor t√°tica
      return this.#escolherMelhorAcao(estado, acoesValidas);
    }
  }

  /**
   * Consulta a "mem√≥ria" e escolhe a a√ß√£o com o maior valor Q.
   * 
   * √â como olhar na "Enciclop√©dia de Monstros" e escolher a t√°tica que
   * j√° provou ser a mais eficaz. Se houver empate (v√°rias t√°ticas igualmente
   * boas), escolhe aleatoriamente entre elas para evitar sempre fazer o mesmo
   * padr√£o de jogadas.
   * 
   * Processo:
   * 1. Avalia o valor Q de todas as a√ß√µes v√°lidas
   * 2. Encontra o maior valor Q
   * 3. Se houver empate, escolhe aleatoriamente entre as melhores
   * 
   * @private
   * @param {string} estado - O estado atual do tabuleiro
   * @param {Array<number>} acoesValidas - Lista de posi√ß√µes dispon√≠veis para jogar (0-8)
   * @returns {number} A melhor a√ß√£o escolhida (posi√ß√£o de 0 a 8 no tabuleiro)
   */
  #escolherMelhorAcao(estado, acoesValidas) {
    const valoresQDasAcoes = {};
    for (const acao of acoesValidas) {
      valoresQDasAcoes[acao] = this.obterValorQ(estado, acao);
    }

    const valorMaximoQ = Math.max(...Object.values(valoresQDasAcoes));
    const melhoresAcoes = Object.entries(valoresQDasAcoes)
      .filter(([_, valor]) => valor === valorMaximoQ)
      .map(([acao]) => parseInt(acao));

    return melhoresAcoes[Math.floor(Math.random() * melhoresAcoes.length)];
  }

  // --- M√âTODOS PARA O CICLO DE TREINAMENTO (GERENCIADOS PELO TREINADOR) ---

  /**
   * Limpa a mem√≥ria de curto prazo para o in√≠cio de uma nova partida.
   * 
   * √â como come√ßar uma nova "quest" no Ragnarok: voc√™ esquece o que aconteceu
   * na √∫ltima e foca totalmente na nova miss√£o.
   * 
   * @returns {void}
   */
  iniciarNovaPartida() {
    this.historicoPartida = [];
  }

  /**
   * Guarda a jogada (estado, a√ß√£o) feita nesta partida.
   * 
   * √â como anotar no "di√°rio de bordo" cada movimento que voc√™ fez na quest,
   * para depois analisar o que deu certo e o que deu errado.
   * 
   * @param {string} estado - O estado do tabuleiro no momento da jogada
   * @param {number} acao - A a√ß√£o (posi√ß√£o) escolhida
   * @returns {void}
   */
  registrarJogada(estado, acao) {
    this.historicoPartida.push([estado, acao]);
  }

  /**
   * Processa o hist√≥rico da partida finalizada para aprender com ela (M√©todo Monte Carlo).
   * 
   * √â como ganhar EXP no Ragnarok: depois da batalha, voc√™ revisa tudo que fez
   * (do fim para o come√ßo) e aprende quais movimentos foram bons ou ruins.
   * O Agente tamb√©m fica menos curioso (epsilon decay) √† medida que ganha experi√™ncia.
   * 
   * O aprendizado acontece de tr√°s pra frente porque:
   * - A √∫ltima jogada teve impacto direto no resultado
   * - Jogadas anteriores tiveram impacto mais indireto (multiplicado por gamma)
   * 
   * Este m√©todo √© chamado pelo Treinador ao final de cada jogo.
   * 
   * @param {number} recompensaFinal - Recompensa final da partida (+1 vit√≥ria, -1 derrota, 0 empate)
   * @returns {void}
   */
  aprenderComFimDePartida(recompensaFinal) {
    this.partidasTreinadas += 1;
    
    if (recompensaFinal > 0) this.vitorias++;
    else if (recompensaFinal < 0) this.derrotas++;
    else this.empates++;

    // Propaga a recompensa final para tr√°s, valorizando as jogadas
    // que levaram a este resultado. Reutiliza o m√©todo 'aprender' para
    // manter a l√≥gica centralizada.
    for (let i = this.historicoPartida.length - 1; i >= 0; i--) {
      const [estado, acao] = this.historicoPartida[i];
      this.aprender(estado, acao, recompensaFinal, estado, true); // finalizado=true
      // A recompensa perde um pouco de for√ßa a cada passo para tr√°s
      recompensaFinal *= this.gamma;
    }

    this.reduzirEpsilon();
  }

  /**
   * Reduz a "curiosidade" do Agente (epsilon decay).
   * 
   * √â como um jogador de Ragnarok que, conforme ganha experi√™ncia,
   * para de explorar mapas aleat√≥rios e foca nas rotas que j√° conhece.
   * O epsilon nunca fica menor que o m√≠nimo configurado.
   * 
   * F√≥rmula: epsilon = max(epsilon_minimo, epsilon √ó taxa_decaimento)
   * 
   * @returns {void}
   */
  reduzirEpsilon() {
    this.epsilon = Math.max(
      this.epsilonMinimo,
      this.epsilon * this.taxaDecaimentoEpsilon
    );
  }

  /**
   * Salva o conhecimento do Agente (a Tabela Q) em um arquivo JSON.
   * 
   * √â como salvar o "save game" no Ragnarok: toda a experi√™ncia e conhecimento
   * adquirido √© preservado para ser usado depois. O diret√≥rio √© criado
   * automaticamente se n√£o existir.
   * 
   * Nota: O salvamento √© silencioso para n√£o poluir o console durante
   * treinamentos em massa com muitos checkpoints.
   * 
   * @param {string} caminho - Caminho onde salvar o arquivo JSON
   * @returns {void}
   * @throws {Error} Se houver problema ao salvar o arquivo
   */
  salvarMemoria(caminho) {
    const caminhoCompleto = path.resolve(caminho);
    const diretorio = path.dirname(caminhoCompleto);

    if (!fs.existsSync(diretorio)) {
      fs.mkdirSync(diretorio, { recursive: true });
    }

    try {
      fs.writeFileSync(
        caminhoCompleto,
        JSON.stringify(this.tabelaQ, null, 2)
      );
    } catch (err) {
      throw new Error(`Erro ao salvar mem√≥ria: ${err.message}`);
    }
  }

  /**
   * Cria uma inst√¢ncia de Agente e carrega seu conhecimento de um arquivo.
   * 
   * √â como carregar um "save game" no Ragnarok: o Agente recupera toda
   * a experi√™ncia e conhecimento que tinha antes. Se o arquivo n√£o existir,
   * o Agente come√ßa do zero (tabela Q vazia).
   * 
   * Permite sobrescrever hiperpar√¢metros no momento do carregamento.
   * 
   * @static
   * @param {string} caminho - Caminho do arquivo JSON contendo a tabela Q
   * @param {Object} kwargs - Hiperpar√¢metros customizados (alpha, gamma, etc.)
   * @returns {AgenteQLearning} Nova inst√¢ncia do agente com mem√≥ria carregada
   */
  static carregar(caminho, kwargs = {}) {
    // Cria um novo agente, passando quaisquer hiperpar√¢metros customizados
    const agente = new AgenteQLearning(kwargs);

    const caminhoCompleto = path.resolve(caminho);

    if (fs.existsSync(caminhoCompleto)) {
      const dados = fs.readFileSync(caminhoCompleto, 'utf-8');
      agente.tabelaQ = JSON.parse(dados);

      console.log(`‚úÖ Mem√≥ria do Agente (${agente.simbolo}) carregada de: ${caminhoCompleto}`);
      console.log(`   - O Agente conhece ${Object.keys(agente.tabelaQ).length.toLocaleString('pt-BR')} situa√ß√µes de jogo.`);
    } else {
      console.log(`‚ö†Ô∏è  Aviso: Nenhum arquivo de mem√≥ria encontrado em ${caminho}. O Agente (${agente.simbolo}) come√ßar√° do zero.`);
    }

    return agente;
  }

  /**
   * Imprime as estat√≠sticas de forma leg√≠vel no console.
   * 
   * √â como abrir a tela de "Character Info" no Ragnarok e ver
   * todos os detalhes do seu personagem formatados de forma bonita.
   * 
   * @returns {void}
   */
  imprimirEstatisticas() {
    const totalJogos = this.vitorias + this.derrotas + this.empates;
    
    let taxaVitoria, taxaEmpate, taxaDerrota;
    if (totalJogos === 0) {
      taxaVitoria = 0.0;
      taxaEmpate = 0.0;
      taxaDerrota = 0.0;
    } else {
      taxaVitoria = this.vitorias / totalJogos;
      taxaEmpate = this.empates / totalJogos;
      taxaDerrota = this.derrotas / totalJogos;
    }

    console.log(`\n${'='.repeat(50)}`);
    console.log(`üìä ESTAT√çSTICAS DO AGENTE (${this.simbolo})`);
    console.log(`${'='.repeat(50)}`);
    console.log(`Partidas treinadas:   ${this.partidasTreinadas.toLocaleString('pt-BR')}`);
    console.log(`Estados conhecidos:   ${Object.keys(this.tabelaQ).length.toLocaleString('pt-BR')}`);
    console.log(`Curiosidade (Epsilon):${this.epsilon.toFixed(4)}`);
    console.log(`\n--- Desempenho ---`);
    console.log(`Vit√≥rias:   ${String(this.vitorias).padStart(6)} (${(taxaVitoria * 100).toFixed(1).padStart(5)}%)`);
    console.log(`Empates:    ${String(this.empates).padStart(6)} (${(taxaEmpate * 100).toFixed(1).padStart(5)}%)`);
    console.log(`Derrotas:   ${String(this.derrotas).padStart(6)} (${(taxaDerrota * 100).toFixed(1).padStart(5)}%)`);
    console.log(`${'='.repeat(50)}\n`);
  }
}
