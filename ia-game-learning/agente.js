/**
 * @module üß† agente.js
 * @project üìò AI Game Learning
 * 
 * Este m√≥dulo define o Agente que utiliza o algoritmo Q-Learning.
 * No paradigma de Aprendizado por Refor√ßo (RL), este c√≥digo representa o "Agent".
 * 
 * Resposabilidades do Agente:
 * - Mante uma "mem√≥ria de jogo", a Tabela Q (Q-Table).
 * - Decidir qual a√ß√£o tomar, balanceando entre explorar e usar seu conhecimento.
 * - Aprender com os resultados de suas a√ß√µes, atualizando sua mem√≥ria.
 */

import fs from "fs";

/**
 * Classe que representa um Agente que aprende a jogar o Jogo da Velha usando Q-Learning.
 * 
 * Pense neste Agente como um jogador de Ragnarok Online que est√° aprendendo
 * a melhor estrat√©gia para derrotar monstros.
 * 
 * Hiperpar√¢metros (os "atributos" do nosso jogador):
 * 
 * @property {number} alpha (Œ±) - A "Velocidade de Aprendizado"
 *   - Qu√£o r√°pido o jogador ajusta sua estrat√©gia ap√≥s uma batalha.
 *   - Valores altos = impulsivo, aprende r√°pido com uma √∫nica experi√™ncia.
 *   - Valores baixos = c√©tico, precisa de muitas experi√™ncias para mudar de ideia.
 * 
 * @property {number} gamma (Œ≥) - A "Vis√£o de Futuro" (Fator de Desconto)
 *   - O quanto o jogador valoriza recompensas futuras.
 *   - Valor alto = estrategista, pensa nos pr√≥ximos passos.
 *   - Valor baixo = imediatista, foca apenas na recompensa de agora.
 * 
 * @property {number} epsilon (Œµ) - O "Medidor de Curiosidade" (Taxa de Explora√ß√£o)
 *   - A chance do jogador tentar uma t√°tica nova e desconhecida.
 *   - Valor alto = aventureiro, adora explorar o mapa.
 *   - Valor baixo = conservador, prefere usar a t√°tica que j√° sabe que funciona.
 */
export class AgenteQLearning {
  /**
   * Cria um novo Agente de Q-Learning para jogar o Jogo da Velha.
   * 
   * √â como criar um personagem novo no Ragnarok: voc√™ define seus atributos iniciais
   * (INT, DEX, etc.) que v√£o determinar como ele vai evoluir durante o jogo.
   * 
   * @param {Object} config - Objeto de configura√ß√£o com os hiperpar√¢metros do agente
   * @param {number} [config.alpha=0.5] - Taxa de aprendizado (0 a 1). Controla a velocidade de aprendizado
   * @param {number} [config.gamma=0.9] - Fator de desconto (0 a 1). Valor dado a recompensas futuras
   * @param {number} [config.epsilon=1.0] - Taxa de explora√ß√£o inicial (0 a 1). Chance de fazer jogadas aleat√≥rias
   * @param {number} [config.epsilonMinimo=0.01] - Valor m√≠nimo que epsilon pode atingir durante o decaimento
   * @param {number} [config.taxaDecaimentoEpsilon=0.9995] - Multiplicador de decaimento aplicado ao epsilon a cada epis√≥dio
   * @param {number} [config.jogador=1] - Identificador do jogador (1 para X, 2 para O)
   */
  constructor({
    alpha = 0.5,
    gamma = 0.9,
    epsilon = 1.0,
    epsilonMinimo = 0.01,
    taxaDecaimentoEpsilon = 0.9995,
    jogador = 1
  } = {}) {
    /** --- HIPERPAR√ÇMETROS (Atributos do Agente) --- */
    this.alpha = alpha;
    this.gamma = gamma;
    this.epsilon = epsilon;
    this.epsilonMinimo = epsilonMinimo;
    this.taxaDecaimentoEpsilon = taxaDecaimentoEpsilon;

    /** --- IDENTIDADE --- */
    this.jogador = jogador;
    this.simbolo = jogador === 1 ? 'X' : 'O';

    /** --- MEM√ìRIA (A "Enciclop√©dia de Monstros" do Jogador) --- */
    // Estrutura: { estado_do_tabuleiro: { acao: valor_q } }
    this.tabelaQ = {};

    /** --- ESTAT√çSTICAS DE TREINO --- */
    this.partidasTreinadas = 0;
    this.vitorias = 0;
    this.derrotas = 0;
    this.empates = 0;
  }


  /**
   * Consulta a "mem√≥ria" (Q-Table) para ver o valor de uma a√ß√£o em um estado.
   * 
   * Se o Agente nunca viu essa situa√ß√£o antes, ele assume que o valor √© 0,
   * como um jogador que nunca enfrentou aquele tipo de monstro e n√£o sabe
   * se a estrat√©gia ser√° boa ou ruim.
   * 
   * @param {Array} estado - O estado atual do jogo (representa√ß√£o do tabuleiro)
   * @param {number} acao - A a√ß√£o (posi√ß√£o de 0 a 8) que o agente quer avaliar
   * @returns {number} O valor Q da a√ß√£o naquele estado (recompensa esperada)
   */
  obterValorQ(estado, acao) {
    const estadoKey = JSON.stringify(estado);

    if (!this.tabelaQ[estadoKey]) {
      this.tabelaQ[estadoKey] = {}
    };

    if (!(acao in this.tabelaQ[estadoKey])) {
      this.tabelaQ[estadoKey][acao] = 0.0;
    }

    return this.tabelaQ[estadoKey][acao];
  }

  /**
 * Atualiza a "mem√≥ria" do Agente usando a Equa√ß√£o de Bellman.
 * √â aqui que o aprendizado realmente acontece!
 * 
 * Pense como se fosse ganhar EXP no Ragnarok: depois de uma batalha,
 * voc√™ aprende se aquela estrat√©gia foi boa ou ruim e ajusta sua
 * "experi√™ncia" com base na recompensa que recebeu e no que espera
 * das pr√≥ximas batalhas.
 * 
 * A Equa√ß√£o de Bellman calcula:
 * Novo Valor Q = Opini√£o Antiga + Taxa de Aprendizado √ó (Surpresa)
 * Onde Surpresa = Valor Real - Opini√£o Antiga
 * 
 * @param {Array} estado - O estado atual do tabuleiro antes da jogada
 * @param {number} acao - A a√ß√£o (posi√ß√£o 0-8) que foi tomada
 * @param {number} recompensa - A recompensa recebida (+1, -1 ou 0)
 * @param {Array} proximoEstado - O estado do tabuleiro ap√≥s a jogada
 * @returns {void}
 */
  atualizarValorQ(estado, acao, recompensa, proximoEstado) {
    const estadoKey = JSON.stringify(estado);

    const opiniaoAntiga = this.obterValorQ(estado, acao);
    const melhorValorFuturo = this.#melhorValorQEstado(proximoEstado);
    const valorRealDaJogada = recompensa + this.gamma * melhorValorFuturo;
    const surpresa = valorRealDaJogada - opiniaoAntiga;
    const novoValorQ = opiniaoAntiga + this.alpha * surpresa;

    if (!this.tabelaQ[estadoKey]) {
      this.tabelaQ[estadoKey] = {}
    };

    this.tabelaQ[estadoKey][acao] = novoValorQ;
  }

  /**
 * Verifica na "mem√≥ria" qual √© a melhor jogada poss√≠vel a partir de um estado.
 * 
 * √â como se o Agente olhasse todas as t√°ticas que ele j√° testou naquela
 * situa√ß√£o e escolhesse aquela que teve o melhor resultado no passado.
 * Se ele nunca viu aquela situa√ß√£o antes, retorna 0 (neutro).
 * 
 * @private
 * @param {Array} estado - O estado do tabuleiro que queremos avaliar
 * @returns {number} O maior valor Q encontrado para aquele estado (melhor recompensa esperada)
 */
  #melhorValorQEstado(estado) {
    const estadoKey = JSON.stringify(estado);
    const acoes = this.tabelaQ[estadoKey];
    if (!acoes || Object.keys(acoes).length === 0) return 0.0;
    return Math.max(...Object.values(acoes));
  }

  /**
 * Decide qual jogada fazer usando a estrat√©gia Epsilon-Greedy.
 * 
 * √â a estrat√©gia que equilibra "Aventura" (explora√ß√£o) e "Farm" (explora√ß√£o).
 * Como um jogador de Ragnarok que √†s vezes sai do caminho conhecido para
 * explorar novos mapas (pode encontrar algo melhor) ou fica no mapa conhecido
 * farmando o que j√° sabe que funciona.
 * 
 * A estrat√©gia funciona assim:
 * - Durante o treinamento: com probabilidade epsilon (Œµ), escolhe a√ß√£o aleat√≥ria
 *   (explora√ß√£o). Caso contr√°rio, escolhe a melhor a√ß√£o conhecida (explora√ß√£o).
 * - Fora do treinamento: sempre escolhe a melhor a√ß√£o conhecida.
 * 
 * @param {Array} estado - O estado atual do tabuleiro
 * @param {Array<number>} acoesValidas - Lista de posi√ß√µes dispon√≠veis para jogar (0-8)
 * @param {boolean} [emTreinamento=true] - Se true, usa epsilon-greedy. Se false, sempre escolhe a melhor a√ß√£o
 * @returns {number} A a√ß√£o escolhida (posi√ß√£o de 0 a 8 no tabuleiro)
 * @throws {Error} Se n√£o houver a√ß√µes v√°lidas dispon√≠veis
 */
escolherAcao(estado, acoesValidas, emTreinamento = true) {
  if (!acoesValidas || acoesValidas.length === 0)
    throw new Error("N√£o h√° a√ß√µes v√°lidas para escolher.");

  if (!emTreinamento)
    return this.#escolherMelhorAcao(estado, acoesValidas);

  if (Math.random() < this.epsilon) {
    // "Modo Aventura": explora
    return acoesValidas[Math.floor(Math.random() * acoesValidas.length)];
  } else {
    // "Modo Farm": usa melhor t√°tica
    return this.#escolherMelhorAcao(estado, acoesValidas);
  }
}

/**
 * Consulta a "mem√≥ria" e escolhe a a√ß√£o com o maior valor Q.
 * 
 * √â como olhar na "Enciclop√©dia de Monstros" e escolher a t√°tica que
 * j√° provou ser a mais eficaz. Se houver empate (v√°rias t√°ticas igualmente
 * boas), escolhe aleatoriamente entre elas para evitar sempre fazer o mesmo
 * padr√£o de jogadas.
 * 
 * Processo:
 * 1. Avalia o valor Q de todas as a√ß√µes v√°lidas
 * 2. Encontra o maior valor Q
 * 3. Se houver empate, escolhe aleatoriamente entre as melhores
 * 
 * @private
 * @param {Array} estado - O estado atual do tabuleiro
 * @param {Array<number>} acoesValidas - Lista de posi√ß√µes dispon√≠veis para jogar (0-8)
 * @returns {number} A melhor a√ß√£o escolhida (posi√ß√£o de 0 a 8 no tabuleiro)
 */
#escolherMelhorAcao(estado, acoesValidas) {
  const valoresQdasAcoes = {};
  for (const acao of acoesValidas) {
    valoresQdasAcoes[acao] = this.obterValorQ(estado, acao);
  }
  const valorMaximoQ = Math.max(...Object.values(valoresQdasAcoes));
  const melhoresAcoes = Object.entries(valoresQdasAcoes)
    .filter(([_, v]) => v === valorMaximoQ)
    .map(([k]) => parseInt(k));

  return melhoresAcoes[Math.floor(Math.random() * melhoresAcoes.length)];
}

/**
 * Processa o hist√≥rico de uma partida finalizada para aprender com ela.
 * 
 * √â como ganhar EXP no Ragnarok: depois da batalha, voc√™ revisa tudo que fez
 * (do fim para o come√ßo) e aprende quais movimentos foram bons ou ruins.
 * O Agente tamb√©m fica menos curioso (epsilon decay) √† medida que ganha experi√™ncia.
 * 
 * O aprendizado acontece de tr√°s pra frente porque:
 * - A √∫ltima jogada teve impacto direto no resultado
 * - Jogadas anteriores tiveram impacto mais indireto (multiplicado por gamma)
 * 
 * @param {Array<Array>} historicoPartida - Array de tuplas [estado, acao, proximo_estado]
 * @param {number} recompensaFinal - Recompensa final da partida (+1 vit√≥ria, -1 derrota, 0 empate)
 * @returns {void}
 */
aprenderJogando(historicoPartida, recompensaFinal) {
  this.partidasTreinadas += 1;
  if (recompensaFinal > 0.5) this.vitorias++;
  else if (recompensaFinal < 0.5) this.derrotas++;
  else this.empates++;

  for (let i = historicoPartida.length - 1; i >= 0; i--) {
    const [estado, acao, proximoEstado] = historicoPartida[i];
    this.atualizarValorQ(estado, acao, recompensaFinal, proximoEstado);
    recompensaFinal *= this.gamma;
  }

  this.reduzirEpsilon();
}

/**
 * Reduz a "curiosidade" do Agente ao longo do tempo (epsilon decay).
 * 
 * √â como um jogador de Ragnarok que, conforme ganha experi√™ncia,
 * para de explorar mapas aleat√≥rios e foca nas rotas que j√° conhece.
 * O epsilon nunca fica menor que o m√≠nimo configurado.
 * 
 * F√≥rmula: epsilon = max(epsilon_minimo, epsilon √ó taxa_decaimento)
 * 
 * @returns {void}
 */
reduzirEpsilon() {
  this.epsilon = Math.max(this.epsilonMinimo, this.epsilon * this.taxaDecaimentoEpsilon);
}

/**
 * Salva o conhecimento do Agente (a Tabela Q) em um arquivo JSON.
 * 
 * √â como salvar o "save game" no Ragnarok: toda a experi√™ncia e conhecimento
 * adquirido √© preservado para ser usado depois. O diret√≥rio √© criado
 * automaticamente se n√£o existir.
 * 
 * @param {string} [caminho="agente_treinado.json"] - Caminho onde salvar o arquivo JSON
 * @returns {void}
 */
salvarMemoria(caminho = "agente_treinado.json") {
  const caminhoCompleto = path.resolve(caminho);
  fs.mkdirSync(path.dirname(caminhoCompleto), { recursive: true });
  fs.writeFileSync(caminhoCompleto, JSON.stringify(this.tabelaQ, null, 2));
  console.log(`üíæ Mem√≥ria do Agente salva em: ${caminhoCompleto}`);
}

/**
 * Carrega o conhecimento de um Agente previamente treinado.
 * 
 * √â como carregar um "save game" no Ragnarok: o Agente recupera toda
 * a experi√™ncia e conhecimento que tinha antes. Se o arquivo n√£o existir,
 * o Agente come√ßa do zero (tabela Q vazia).
 * 
 * @param {string} caminho - Caminho do arquivo JSON contendo a tabela Q
 * @returns {void}
 */
carregarMemoria(caminho) {
  const caminhoCompleto = path.resolve(caminho);
  if (!fs.existsSync(caminhoCompleto)) {
    console.log(`‚ö†Ô∏è  Aviso: Nenhum arquivo de mem√≥ria encontrado em ${caminho}. O Agente come√ßar√° do zero.`);
    return;
  }

  this.tabelaQ = JSON.parse(fs.readFileSync(caminhoCompleto, 'utf-8'));
  console.log(`‚úÖ Mem√≥ria do Agente carregada de: ${caminhoCompleto}`);
  console.log(`   - O Agente conhece ${Object.keys(this.tabelaQ).length.toLocaleString()} situa√ß√µes de jogo.`);
}






}